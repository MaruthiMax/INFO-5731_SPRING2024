{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaruthiMax/INFO-5731_SPRING2024/blob/main/lingampalli_maruthi_Exercis_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c691384f-2041-430a-e2a1-a26f9f7ab4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9eXMVBfP25N",
        "outputId": "d728ca12-739b-4ca8-c975-79d9a46f8106"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.9.0)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: funcy, tzdata, pandas, pyLDAvis\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 pandas-2.2.1 pyLDAvis-3.4.1 tzdata-2024.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary liraries\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "#downloading pacages punkt and stopwords\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUrJBMXTP3Rc",
        "outputId": "2d976348-023d-441c-bf76-db93c4d55b83"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# taking a article text for the model\n",
        "article = \"\"\"\n",
        "In recent years, the fashion industry has faced growing scrutiny over its environmental impact, from excessive water usage to the disposal of unsold clothing in landfills. Enter GreenThreads, a visionary company poised to tackle these challenges head-on. Founded by a group of environmental enthusiasts and fashion designers, GreenThreads has embarked on a mission to redefine sustainable fashion.\n",
        "\n",
        "At the core of GreenThreads' business model is its commitment to using only eco-friendly materials. Organic cotton, known for its minimal environmental footprint, is the primary fabric in their collections. By avoiding the pesticides and synthetic fertilizers used in conventional cotton farming, GreenThreads significantly reduces water pollution and health hazards to workers. Furthermore, the company has innovated a line of clothing made from bamboo fibers, which boast natural antibacterial properties and require less water to grow compared to other crops.\n",
        "\n",
        "But GreenThreads doesn't stop there. Recognizing the problem of plastic waste, the company has developed a process for creating recycled polyester from plastic bottles, giving new life to what would otherwise be pollution. This initiative not only cleans up the environment but also provides customers with durable and water-resistant clothing options.\n",
        "\n",
        "Transparency is another cornerstone of GreenThreads' philosophy. The company meticulously tracks and shares its supply chain details, from the sourcing of raw materials to the ethical treatment of workers in its factories. This level of openness has cultivated a loyal customer base that values the integrity behind each purchase.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uV3ZrlQCRL0",
        "outputId": "06d92ddb-baf9-4e1b-8d86-e3f4ef421894"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# function for preprocessing given text\n",
        "def preprocess_of_the_text(article):\n",
        "    # Removed  any punctuation marks and making all the text to lower case\n",
        "    article = article.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    #making small words(tokens)\n",
        "    tokens_a = word_tokenize(article)\n",
        "    #removing stop words from the\n",
        "    stop_words_a = set(stopwords.words('english'))\n",
        "    tokens_a = [b for b in tokens_a if b not in stop_words_a]\n",
        "    return tokens_a\n",
        "\n",
        "\n",
        "# Tokenize and preprocess the text\n",
        "tokens_of_text = preprocess_of_the_text(article)\n",
        "dict_of_text = corpora.Dictionary([tokens_of_text])\n",
        "corp_of_text = [dict_of_text.doc2bow(tokens_of_text)]\n",
        "\n",
        "# defining a function for calucuate coherence scores for eaxh topic of the article given as input\n",
        "def coher_scores_of_text(corp_of_text, dict_of_text, article_text, limit, start=2, step=1):\n",
        "    coher_scores = []\n",
        "    for no_of_topics in range(start, limit, step):\n",
        "        text_model = gensim.models.LdaModel(corpus=corp_of_text, id2word=dict_of_text, num_topics=no_of_topics)\n",
        "        coher_of_emodel = CoherenceModel(model=text_model, texts=article_text, dictionary=dict_of_text, coherence='c_v')\n",
        "        textmodel_coherence_score = coher_of_emodel.get_coherence()\n",
        "        coher_scores.append((no_of_topics, textmodel_coherence_score))\n",
        "        print(f'The coherence score for {no_of_topics} topics: {textmodel_coherence_score:.4f}')\n",
        "    return coher_scores\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of8yNRP_6IAR",
        "outputId": "c36d4b28-38bc-4223-bf9b-15f75a09b09a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we are defining the main function of the model\n",
        "def main():\n",
        "    article_limit = 15\n",
        "    global lda_model\n",
        "    coher_scores = coher_scores_of_text(corp_of_text, dict_of_text, [tokens_of_text], limit=article_limit, start=2, step=1)\n",
        "    k_no_of_topics, _= max(coher_scores, key=lambda x: x[1])\n",
        "    lda_model = gensim.models.LdaModel(corpus=corp_of_text, id2word=dict_of_text, num_topics=k_no_of_topics)\n",
        "    topic_article = lda_model.show_topics(formatted=False)\n",
        "\n",
        "    print(f' number of topics k is : {k_no_of_topics}')\n",
        "\n",
        "    for id, topic in topic_article:\n",
        "        words_text = [words_text for words_text, weight in topic]\n",
        "        top_words = \", \".join([f\"{words_text} ({weight:.4f})\" for words_text, weight in topic])\n",
        "        print(f'the topic {id + 1}: {top_words}')\n",
        "\n",
        "\n",
        "# here we are calling the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# we are visulaizing the topics and their coherence importance in topic selction\n",
        "visualise_data = pyLDAvis.gensim.prepare(lda_model, corp_of_text, dict_of_text)\n",
        "pyLDAvis.display(visualise_data)"
      ],
      "metadata": {
        "id": "QW6PcYXd6IkK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e78d0b5-4d34-45a9-dfc8-9535aa38345f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 2 topics: 0.6088\n",
            "The coherence score for 3 topics: 0.5519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 4 topics: 0.5548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 5 topics: 0.5480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 6 topics: 0.5655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 7 topics: 0.5196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 8 topics: 0.5195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 9 topics: 0.5862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 10 topics: 0.5369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 11 topics: 0.5555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 12 topics: 0.5526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 13 topics: 0.5430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence score for 14 topics: 0.5018\n",
            " number of topics k is : 2\n",
            "the topic 1: greenthreads (0.0249), company (0.0175), water (0.0172), fashion (0.0165), clothing (0.0154), environmental (0.0150), cotton (0.0122), workers (0.0119), pollution (0.0114), plastic (0.0103)\n",
            "the topic 2: greenthreads (0.0264), company (0.0191), environmental (0.0143), clothing (0.0139), fashion (0.0128), materials (0.0123), water (0.0121), plastic (0.0117), pollution (0.0105), workers (0.0100)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el83191328673950442881271774288\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el83191328673950442881271774288_data = {\"mdsDat\": {\"x\": [0.0010628927816161904, -0.0010628927816161904], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [64.58743810653687, 35.412561893463135]}, \"tinfo\": {\"Term\": [\"greenthreads\", \"company\", \"materials\", \"mission\", \"plastic\", \"philosophy\", \"environment\", \"environmental\", \"enthusiasts\", \"primary\", \"treatment\", \"details\", \"using\", \"excessive\", \"sourcing\", \"scrutiny\", \"sustainable\", \"require\", \"developed\", \"cleans\", \"transparency\", \"waterresistant\", \"business\", \"designers\", \"stop\", \"collections\", \"clothing\", \"commitment\", \"fibers\", \"new\", \"water\", \"provides\", \"less\", \"fashion\", \"values\", \"raw\", \"cotton\", \"customer\", \"tracks\", \"redefine\", \"ecofriendly\", \"process\", \"initiative\", \"giving\", \"workers\", \"conventional\", \"openness\", \"made\", \"disposal\", \"unsold\", \"farming\", \"life\", \"visionary\", \"clothing\", \"properties\", \"pollution\", \"recognizing\", \"waste\", \"chain\", \"loyal\", \"environmental\", \"greenthreads\", \"company\", \"plastic\", \"materials\", \"mission\", \"materials\", \"philosophy\", \"environment\", \"enthusiasts\", \"primary\", \"treatment\", \"details\", \"using\", \"excessive\", \"sourcing\", \"scrutiny\", \"sustainable\", \"require\", \"developed\", \"cleans\", \"transparency\", \"waterresistant\", \"business\", \"designers\", \"plastic\", \"stop\", \"collections\", \"commitment\", \"fibers\", \"new\", \"shares\", \"durable\", \"hazards\", \"crops\", \"company\", \"greenthreads\", \"environmental\", \"clothing\", \"pollution\", \"fashion\", \"water\", \"workers\", \"cotton\"], \"Freq\": [3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.6293916465367613, 0.8064516995915529, 0.8034559264358612, 1.5640247251409096, 0.7769976257647067, 0.7746609580724897, 1.1546852167926631, 0.7686204253877811, 0.7653647768689081, 0.7624918233387552, 0.7596258552300692, 0.7588588736376114, 0.7583624665981765, 0.7564244100448574, 1.1327091692807523, 0.7525456442465228, 0.7515354992485705, 0.7502460258149976, 0.7463329518707241, 0.743741625775662, 0.7393078284514043, 0.735897881698863, 0.7320612936984997, 1.4632358809859818, 0.7301880070340987, 1.086868446192219, 0.7241156862605627, 0.7228064060623239, 0.7212975108139383, 0.7209416522228793, 1.423395811783656, 2.361210572637275, 1.665815933064814, 0.9784069565749036, 0.9173106035409305, 0.44846898450690076, 0.6424439801589266, 0.42721587051028986, 0.42641297067544937, 0.418078976079174, 0.41796717805857486, 0.4176318809595013, 0.41674418722265394, 0.4139124878353365, 0.41355406512662474, 0.41321425926089217, 0.4125313023154542, 0.4113032694178931, 0.41117270911022896, 0.40924576889915654, 0.40877520880031215, 0.40832748342293446, 0.4082100130829901, 0.4080428978284345, 0.40676817737933957, 0.6083156223945223, 0.4050117945995194, 0.40365320139429417, 0.4018264721583208, 0.4017822329155642, 0.40081549031809804, 0.3999778778281192, 0.3994653571105287, 0.39916983896891467, 0.3991105947446423, 0.9942260028023877, 1.3756769705776646, 0.7446898634620526, 0.722436094155309, 0.547726234254158, 0.6661333795549943, 0.6296253141404997, 0.5221159063358966, 0.5098432858968314], \"Total\": [3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.259016960677261, 1.1258695769903808, 1.1245492227400944, 2.230158104695904, 1.1128692289770576, 1.1118401763380017, 1.6645285026894945, 1.1091740191653185, 1.1077340461318999, 1.1064684194343286, 1.105204227042284, 1.1048660146146911, 1.1046456604500574, 1.1037904351153338, 1.6548250756166487, 1.1020786649916436, 1.1016321158260698, 1.1010639723358984, 1.0993360260956293, 1.0981935897423778, 1.0962338690391973, 1.094727930813538, 1.0930365703563145, 2.1856719751412905, 1.0922092205940843, 1.634594680446377, 1.0895279529210757, 1.0889521714880819, 1.0882843793877337, 1.0881288700248297, 2.1680856752457087, 3.73688754321494, 2.6600419358672016, 1.586722578969426, 1.5597545836998572, 1.0239017433049504, 1.5597545836998572, 1.0406976010773215, 1.0413302495654637, 1.0479149488576693, 1.0479985657920312, 1.048272326706429, 1.0489684225468594, 1.0512050443375553, 1.0514915351917433, 1.0517575156863899, 1.0522967864400234, 1.0532675696895282, 1.0533725241929677, 1.054895497955994, 1.055270439123818, 1.055617990716992, 1.0557091103674336, 1.0558470546579528, 1.0568532511639241, 1.586722578969426, 1.0582429607359196, 1.0593140562783951, 1.0607599085766228, 1.060793393200568, 1.0615551887239412, 1.0622147112158276, 1.0626226745168543, 1.0628553072919742, 1.0629036297159868, 2.6600419358672016, 3.73688754321494, 2.1680856752457087, 2.1856719751412905, 1.634594680446377, 2.230158104695904, 2.259016960677261, 1.6548250756166487, 1.6645285026894945], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.0651, -4.7684, -4.7721, -4.106, -4.8056, -4.8086, -4.4095, -4.8164, -4.8207, -4.8244, -4.8282, -4.8292, -4.8299, -4.8324, -4.4287, -4.8376, -4.8389, -4.8406, -4.8459, -4.8493, -4.8553, -4.8599, -4.8652, -4.1726, -4.8677, -4.47, -4.8761, -4.8779, -4.88, -4.8805, -4.2002, -3.6941, -4.043, -4.5751, -4.6396, -4.7542, -4.3948, -4.8028, -4.8047, -4.8244, -4.8247, -4.8255, -4.8276, -4.8344, -4.8353, -4.8361, -4.8378, -4.8408, -4.8411, -4.8458, -4.8469, -4.848, -4.8483, -4.8487, -4.8518, -4.4494, -4.8562, -4.8595, -4.8641, -4.8642, -4.8666, -4.8687, -4.87, -4.8707, -4.8708, -3.9581, -3.6334, -4.2471, -4.2775, -4.5543, -4.3586, -4.415, -4.6022, -4.626], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.1104, 0.1035, 0.1009, 0.0823, 0.0779, 0.0758, 0.0714, 0.0704, 0.0674, 0.0648, 0.0622, 0.0615, 0.061, 0.0592, 0.0581, 0.0557, 0.0547, 0.0535, 0.0499, 0.0474, 0.0432, 0.04, 0.0363, 0.0359, 0.0345, 0.0291, 0.0286, 0.0273, 0.0258, 0.0255, 0.0164, -0.0219, -0.0309, -0.0464, -0.0937, 0.2126, 0.1511, 0.1477, 0.1453, 0.1192, 0.1189, 0.1178, 0.115, 0.1061, 0.1049, 0.1039, 0.1017, 0.0978, 0.0974, 0.0912, 0.0897, 0.0883, 0.0879, 0.0874, 0.0833, 0.0794, 0.0777, 0.0733, 0.0674, 0.0672, 0.0641, 0.0614, 0.0597, 0.0588, 0.0586, 0.054, 0.0388, -0.0305, -0.0689, -0.0553, -0.1702, -0.2395, -0.1155, -0.1451]}, \"token.table\": {\"Topic\": [1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2], \"Freq\": [0.9471068708184778, 0.9188774725982907, 0.9476243841628801, 0.45752519654069135, 0.45752519654069135, 0.9440071091978345, 0.9427203949872566, 0.7518678457781452, 0.3759339228890726, 0.907376244333323, 0.6007707277972293, 0.6007707277972293, 0.9408190658519108, 0.9015717846984238, 0.9462051603651586, 0.9533175436988219, 0.9479611979932026, 0.9096399792805587, 0.9410678164332159, 0.9048101477825248, 0.9542759181841033, 0.9603101421641114, 0.46123638535948086, 0.46123638535948086, 0.9510300050277117, 0.9122141070832429, 0.8967974045376987, 0.44839870226884937, 0.9426906374132427, 0.9059690754572548, 0.5352047598091081, 0.26760237990455404, 0.9408618399317948, 0.9052676670929731, 0.8892452013468866, 0.9134689742106591, 0.9190087934870999, 0.9082124428052151, 0.6411265018551339, 0.6411265018551339, 0.9766562138786868, 0.9420141417254674, 0.9077440514251348, 0.9608939224658617, 0.6302298922660435, 0.6302298922660435, 0.6117724546411217, 0.6117724546411217, 0.954199779123022, 0.9050871207661665, 0.9155754970243439, 0.8882023463793656, 0.8994098444019512, 0.9178286773817532, 0.9037763594836629, 0.9493317672835082, 0.9503022463682074, 0.9414292510177951, 0.9507894976603878, 0.9449625814705004, 0.9494263649404587, 0.9027437619091905, 0.9473123883771483, 0.9539505856668983, 0.9105862657917966, 0.9512891946120525, 0.8985781742920448, 0.9148824724812401, 0.9183139775858784, 0.8853408517129473, 0.44267042585647365, 0.9472306245912339, 0.6042934777425724, 0.6042934777425724], \"Term\": [\"business\", \"chain\", \"cleans\", \"clothing\", \"clothing\", \"collections\", \"commitment\", \"company\", \"company\", \"conventional\", \"cotton\", \"cotton\", \"crops\", \"customer\", \"designers\", \"details\", \"developed\", \"disposal\", \"durable\", \"ecofriendly\", \"enthusiasts\", \"environment\", \"environmental\", \"environmental\", \"excessive\", \"farming\", \"fashion\", \"fashion\", \"fibers\", \"giving\", \"greenthreads\", \"greenthreads\", \"hazards\", \"initiative\", \"less\", \"life\", \"loyal\", \"made\", \"materials\", \"materials\", \"mission\", \"new\", \"openness\", \"philosophy\", \"plastic\", \"plastic\", \"pollution\", \"pollution\", \"primary\", \"process\", \"properties\", \"provides\", \"raw\", \"recognizing\", \"redefine\", \"require\", \"scrutiny\", \"shares\", \"sourcing\", \"stop\", \"sustainable\", \"tracks\", \"transparency\", \"treatment\", \"unsold\", \"using\", \"values\", \"visionary\", \"waste\", \"water\", \"water\", \"waterresistant\", \"workers\", \"workers\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el83191328673950442881271774288\", ldavis_el83191328673950442881271774288_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el83191328673950442881271774288\", ldavis_el83191328673950442881271774288_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el83191328673950442881271774288\", ldavis_el83191328673950442881271774288_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, preprocess_string, strip_short, stem_text\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Sample data\n",
        "sample_text = [\n",
        "    \"The briyani in irving tastes so good, but the prices are a bit high compared to Inida\",\n",
        "    \"Envinorment is becoming worse day by day due to pollution caused by planes and automobiles.\",\n",
        "    \" India soon to launch UPI method pay method across the worls with its digital currrency policy\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "cclLNMd76InC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b50c85-5a29-4e62-bfb4-50c81956c7df"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creat a DataFrame for the sample text\n",
        "df = pd.DataFrame({'Review': sample_text})\n",
        "\n",
        "# Preprocess the text using the prepocess the text function\n",
        "def preprocess_the_text(text):\n",
        "    cus_filters = [lambda x: x.lower(), remove_stopwords, strip_punctuation, strip_short, stem_text]\n",
        "    text = preprocess_string(text, cus_filters)\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing to the sample text\n",
        "df['Review_Text (Clean)'] = df['Review'].apply(lambda x: preprocess_the_text(x))\n",
        "\n",
        "# Create a dictionary with the corpus\n",
        "corpus_text = df['Review_Text (Clean)']\n",
        "dictionary_text = corpora.Dictionary(corpus_text)\n",
        "\n",
        "# Convert corpus into a bag of words\n",
        "bag_of_words = [dictionary_text.doc2bow(text) for text in corpus_text]"
      ],
      "metadata": {
        "id": "Phscw8As6Ip5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f69ca7b-f828-4516-cd7c-7aea6a0c0006"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the coherencve scores for each text in the given sentences\n",
        "for i in range(2, 11):\n",
        "    lsi = LsiModel(bag_of_words, num_topics=i, id2word=dictionary_text)\n",
        "    coherence_of_model = CoherenceModel(model=lsi, texts=df['Review_Text (Clean)'], dictionary=dictionary_text, coherence='c_v')\n",
        "    coherence_scores = coherence_of_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_scores))\n",
        "\n",
        "# Perform SVD on the bag of words with the LsiModel to extract 2 topics\n",
        "lsi = LsiModel(bag_of_words, num_topics=2, id2word=dictionary_text)\n",
        "\n",
        "# Find the 5 words with the strongest association to the derived topics\n",
        "for topics, words_in_topic in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topics, words_in_topic))\n",
        "\n",
        "# scores for the text\n",
        "corpus_text\n"
      ],
      "metadata": {
        "id": "cxramFJIUH7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f856353f-d383-44d8-b943-c54b56030751"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 2 clusters: 0.1975266915775667\n",
            "Coherence score with 3 clusters: 0.26384705173352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.3041857672520198\n",
            "Coherence score with 5 clusters: 0.29716307953015014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 6 clusters: 0.2971630795301503\n",
            "Coherence score with 7 clusters: 0.29716307953015025\n",
            "Coherence score with 8 clusters: 0.26675594578448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 9 clusters: 0.2637444885168703\n",
            "Coherence score with 10 clusters: 0.23063559004721845\n",
            "Words in 0: 0.555*\"method\" + 0.277*\"pai\" + 0.277*\"digit\" + 0.277*\"india\" + 0.277*\"launch\" + 0.277*\"currrenc\" + 0.277*\"upi\" + 0.277*\"soon\" + 0.277*\"worl\" + 0.277*\"polici\".\n",
            "Words in 1: -0.632*\"dai\" + -0.316*\"automobil\" + -0.316*\"wors\" + -0.316*\"pollut\" + -0.316*\"plane\" + -0.316*\"caus\" + -0.316*\"envinor\" + -0.000*\"compar\" + 0.000*\"bit\" + 0.000*\"briyani\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
            "  sparsetools.csc_matvecs(\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/linalg/_special_matrices.py:154: DeprecationWarning: 'tri'/'tril/'triu' are deprecated as of SciPy 1.11.0 and will be removed in v1.13.0. Please use numpy.(tri/tril/triu) instead.\n",
            "  out = (1 - tri(m.shape[0], m.shape[1], k - 1, m.dtype.char)) * m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [briyani, irv, tast, good, price, bit, high, c...\n",
              "1    [envinor, wors, dai, dai, pollut, caus, plane,...\n",
              "2    [india, soon, launch, upi, method, pai, method...\n",
              "Name: Review_Text (Clean), dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraies\n",
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#initializing count vectorizer\n",
        "vector_text = CountVectorizer()\n",
        "\n",
        "# we are using the article as sample text used in previously\n",
        "a= vector_text.fit_transform(sample_text)\n",
        "\n",
        "# Get the top words for each topic\n",
        "count_words = 10\n",
        "topwords_in_topic= {}\n",
        "for b in range(b.shape[0]):\n",
        "  word_index = np.argsort(x[b].toarray()[0])[::-1][:count_words]\n",
        "  topword = [vector_text.get_feature_names_out()[k] for k in word_index]\n",
        "  out = 'Topic %i has top words: %s' % (k, ', '.join(topword))\n",
        "  print(out)\n",
        "  topwords_in_topic[k] = topword"
      ],
      "metadata": {
        "id": "jaFg3pHzUIiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89429f9-2298-4a2e-f9ad-356ffe674cf8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0 has top words: the, briyani, prices, irving, inida, but, bit, in, so, high\n",
            "Topic 1 has top words: by, day, pollution, envinorment, due, is, caused, planes, worse, becoming\n",
            "Topic 2 has top words: method, india, policy, currrency, digital, worls, its, launch, pay, across\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUxexG49Y8UL",
        "outputId": "3a86bbf3-6e70-4fbb-a1ae-89a3981a5e85"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.25.2)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hdbscan, umap-learn\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039284 sha256=a2d16f56e50b6f40388c9e64c5f19df3e2cb757084f07f574b00ce99ad6557b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=4d572b4d03dc599b3589325285026452758dec2d092a26184c9878932ed0fc0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built hdbscan umap-learn\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cython, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.9\n",
            "    Uninstalling Cython-3.0.9:\n",
            "      Successfully uninstalled Cython-3.0.9\n",
            "Successfully installed bertopic-0.16.0 cython-0.29.37 hdbscan-0.8.33 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pynndescent-0.5.12 sentence-transformers-2.6.1 umap-learn-0.5.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_rand = [\n",
        "    \"Transportation sector known significant contribution pollution seen surge adoption sustainable mobility solution electric bus car sharing micro mobility emerged key player reshaping navigate urban landscape.\",\n",
        "    \"Car sharing program enable individual access vehicle needed basis eliminating need personal car ownership.\",\n",
        "    \"Benefit car sharing extend beyond environmental conservation include reduced traffic congestion minimized parking space requirement lower overall transportation cost user.\",\n",
        "    \"Continued innovation investment advocacy pave way future mobility efficient accessible also sustainable environmentally conscious.\",\n",
        "    \"Furthermore incorporation electric hybrid vehicle within car sharing fleet bolstered sustainability quotient offering user eco friendly alternative transportation need.\",\n",
        "    \"Fostering community empowerment education engagement form cornerstone successful sustainable mobility solution.\",\n",
        "    \"Challenge opportunity horizon.\",\n",
        "    \"Investment research development coupled strategic partnership government business community pave way seamless integration sustainable mobility solution fabric urban life.\",\n",
        "    \"Embracing capability electric bus car sharing micro mobility enable city cultivate transportation ecosystem cleaner efficient also inclusive benefiting environment people inhabit.\",\n",
        "    \"Moreover eco friendly nature resonates increasing focus curbing carbon emission fostering sustainable development urban area.\",\n",
        "    \"Educational program awareness campaign instrumental cultivating culture sustainability encouraging individual make informed choice transportation habit.\",\n",
        "    \"Despite evident benefit sustainable mobility solution several challenge persist widespread implementation.\",\n",
        "    \"Designed short distance travel alternative provide convenient eco conscious solution navigating congested urban area.\",\n",
        "    \"Bustling city hum traditional bus gradually replaced whisper electric bus vehicle represent fundamental shift public transportation offering environmentally friendly alternative gas guzzling counterpart.\",\n",
        "    \"Role infrastructure policy promoting sustainable mobility.\",\n",
        "    \"Rising popularity micro mobility option attributed accessibility flexibility user friendly nature.\",\n",
        "    \"Incorporating community input planning development sustainable mobility project city ensure effort effective also inclusive community centric.\",\n",
        "    \"Proliferation solution positive step success contingent presence appropriate infrastructure well designed policy.\",\n",
        "    \"Furthermore integration smart technology data driven solution hold potential optimize efficiency public transportation network enhance user experience car sharing service streamline operation micro mobility initiative.\",\n",
        "    \"Furthermore policy initiative aimed incentivizing use sustainable mobility solution tax benefit electric vehicle user integration car sharing service urban planning strategy significantly influence consumer behavior encourage transition towards sustainable transportation practice.\"\n",
        "]\n",
        "\n",
        "# Now 'selected_sentences' is a Python list containing the 20 random sentences.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohhnva0zd4bh",
        "outputId": "55f78eba-e5ae-4314-fc3b-dadd8d730d3a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the bert library\n",
        "from bertopic import BERTopic\n",
        "\n",
        "\n",
        "#crearing the bert model below\n",
        "BERT_model = BERTopic(nr_topics=2, calculate_probabilities=True, verbose=True)\n",
        "\n",
        "\n",
        "topic, _ = BERT_model.fit_transform(text_rand)\n",
        "\n",
        "model_overview = BERT_model.get_topic_freq()\n",
        "\n",
        "for n, b in model_overview[1:].values:\n",
        "    bert_model_words = BERT_model.get_topic(n)\n",
        "    bert_model_summary = \", \".join([word[0] for word in bert_model_words[:5]])\n",
        "    print(f\"Topic {n}: {bert_model_summary} (Freq: {b})\")"
      ],
      "metadata": {
        "id": "CK0zgNxcUIkr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "0bad085557a6489c96dcfda15ddca8c7",
            "64bd7dc8a8b84ccfb285579635d8e2c1",
            "8ee43ca3819e456d9feeae949be275de",
            "d53ab1d6baff431b8f34538749b05bc8",
            "befaa096751e4e8ab8e78362e6ecd053",
            "c1082663b61c4faa9b37f739eb107590",
            "1fa5009e3ecd486488e1266d2443e19b",
            "ceb740e7210d468390498dd81764506b",
            "64004465b712421f867394734e93496e",
            "33b0668ef6c34fbba21d73dca879869e",
            "99e5fcc8202143e09f252b5e95b8f242"
          ]
        },
        "outputId": "22dce7b2-d67a-46a1-b98d-972153b38a40"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "2024-03-30 03:48:09,349 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bad085557a6489c96dcfda15ddca8c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 03:48:11,451 - BERTopic - Embedding - Completed ✓\n",
            "2024-03-30 03:48:11,456 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-03-30 03:48:14,457 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-03-30 03:48:14,466 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "/usr/local/lib/python3.10/dist-packages/hdbscan/hdbscan_.py:1170: DeprecationWarning: `alltrue` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `all` instead.\n",
            "  self._all_finite = is_finite(X)\n",
            "2024-03-30 03:48:14,474 - BERTopic - Cluster - Completed ✓\n",
            "2024-03-30 03:48:14,478 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-03-30 03:48:14,501 - BERTopic - Representation - Completed ✓\n",
            "2024-03-30 03:48:14,506 - BERTopic - Topic reduction - Reducing number of topics\n",
            "2024-03-30 03:48:14,509 - BERTopic - Topic reduction - Reduced number of topics from 1 to 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2K_tghoxYp3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CE_gqT5eYpzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdAT2B6EYpni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2TX4nw9uYpkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfiF1u2CUIoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBIEnJIBP3Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\"\"\"\n",
        "BERTopic makes recourse to BERT embeddings to identify refinements and deep semantic similarities in the written content that might pass unobserved by typical bag-of-words models. If dealing with intricate or complex information in texts, approach may end up in subjects deemed far more significant and cohesive.\n",
        "\n",
        "In model analysing and In utilizing the vast knowledge of the pre-trained BERT framework, it can adjust to a broad range of domains and languages.\n",
        "\n",
        "And also BERTopic, tends to yield better defined areas which can be more straightforward for individuals to recognize as topic compared to often more abstract topics generated via systems for instance LDA.\n",
        "\n",
        "At the cutting edge of NLP, it's an updated approach for exposed modeling that can take advantages from ongoing innovations in language models.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OK34nZtojhmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "3b1f17c0-da31-4d31-8a4d-84b16e96e317"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBERTopic makes recourse to BERT embeddings to identify refinements and deep semantic similarities in the written content that might pass unobserved by typical bag-of-words models. If dealing with intricate or complex information in texts, approach may end up in subjects deemed far more significant and cohesive.\\n\\nIn model analysing and In utilizing the vast knowledge of the pre-trained BERT framework, it can adjust to a broad range of domains and languages.\\n\\nAnd also BERTopic, that utilizes HDBSCAN for clustering, tends to yield better defined areas which can be more straightforward for individuals to recognize as topic compared to often more abstract topics generated via systems for instance LDA.\\n\\nAt the cutting edge of NLP, it's an updated approach for exposed modeling that can take advantages from ongoing innovations in language models.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "My learning experience from this assignment is really vast and also I have learned many new things, but the most important thing in this assignment is very difficult to understand the story of what is going on like either many other codes, and online sources, and even I went through stock overflow so some some challenges I facedreally really hard for me to tackle and I feel that this assignment will help me in learning about birth model, very inept and yes, this exercise really helps and also relates to the field of NLP even for my project work where I use the boat model and it also\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "87b44e4c-6b36-41b2-844b-79b72f30f0c7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nMy learning experience from this assignment is really vast and also I have learned many new things, but the most important thing in this assignment is very difficult to understand the story of what is going on like either many other codes, and online sources, and even I went through stock overflow so some some challenges I facedreally really hard for me to tackle and I feel that this assignment will help me in learning about birth model, very inept and yes, this exercise really helps and also relates to the field of NLP even for my project work where I use the boat model and it also\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bad085557a6489c96dcfda15ddca8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64bd7dc8a8b84ccfb285579635d8e2c1",
              "IPY_MODEL_8ee43ca3819e456d9feeae949be275de",
              "IPY_MODEL_d53ab1d6baff431b8f34538749b05bc8"
            ],
            "layout": "IPY_MODEL_befaa096751e4e8ab8e78362e6ecd053"
          }
        },
        "64bd7dc8a8b84ccfb285579635d8e2c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1082663b61c4faa9b37f739eb107590",
            "placeholder": "​",
            "style": "IPY_MODEL_1fa5009e3ecd486488e1266d2443e19b",
            "value": "Batches: 100%"
          }
        },
        "8ee43ca3819e456d9feeae949be275de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb740e7210d468390498dd81764506b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64004465b712421f867394734e93496e",
            "value": 1
          }
        },
        "d53ab1d6baff431b8f34538749b05bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b0668ef6c34fbba21d73dca879869e",
            "placeholder": "​",
            "style": "IPY_MODEL_99e5fcc8202143e09f252b5e95b8f242",
            "value": " 1/1 [00:00&lt;00:00,  2.73it/s]"
          }
        },
        "befaa096751e4e8ab8e78362e6ecd053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1082663b61c4faa9b37f739eb107590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fa5009e3ecd486488e1266d2443e19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceb740e7210d468390498dd81764506b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64004465b712421f867394734e93496e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33b0668ef6c34fbba21d73dca879869e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99e5fcc8202143e09f252b5e95b8f242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}